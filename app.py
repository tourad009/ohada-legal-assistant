# app.py
import streamlit as st
from rag_pipeline import generate_answer_stream, rag_chain

# Configuration de la page
st.set_page_config(
    page_title="OHADA Legal Assistant",
    page_icon="âš–ï¸",
    layout="wide"
)

# Initialisation de l'historique du chat
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# CSS pour styliser les messages de chat
st.markdown("""
<style>
    div.stMarkdown > div {
        padding: 10px;
        border-radius: 10px;
    }
    .stChatMessage.user div {
        background-color: #DCF8C6;
    }
    .stChatMessage.assistant div {
        background-color: #E3F2FD;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.title("OhadAI âš–ï¸")
st.subheader("Votre assistant juridique spÃ©cialisÃ© en droit OHADA")
st.markdown("Posez vos questions juridiques et obtenez des rÃ©ponses prÃ©cises basÃ©es sur les textes OHADA.")

# Affichage de l'historique du chat
chat_container = st.container(height=500)
with chat_container:
    for speaker, message in st.session_state.chat_history:
        role = "user" if speaker == "User" else "assistant"
        with st.chat_message(role, avatar="ğŸ‘¤" if role == "user" else "ğŸ¤–"):
            st.markdown(message)

# EntrÃ©e utilisateur avec st.chat_input
user_question = st.chat_input(
    placeholder="Posez votre question juridique... Ex: Quelles sont les Ã©tapes pour crÃ©er une SARL selon l'OHADA ?"
)

# Traitement de la question
if user_question and user_question.strip():
    # Ajouter la question de l'utilisateur Ã  l'historique
    st.session_state.chat_history.append(("User", user_question))
    
    # Afficher le message de l'utilisateur
    with chat_container:
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(user_question)
    
    # GÃ©nÃ©rer et afficher la rÃ©ponse en streaming
    with chat_container:
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            message_placeholder = st.empty()
            full_response = ""
            for chunk in generate_answer_stream(user_question, rag_chain):
                full_response += chunk
                message_placeholder.markdown(full_response)
    
    # Ajouter la rÃ©ponse complÃ¨te Ã  l'historique
    st.session_state.chat_history.append(("Assistant", full_response))