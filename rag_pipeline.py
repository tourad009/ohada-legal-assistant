import os
import zipfile
from huggingface_hub import snapshot_download
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from dotenv import load_dotenv
from pydantic import SecretStr

load_dotenv()


# Download and unzip vectorstore
def setup_vectorstore():
    local_dir = "ohada_vectorstore"
    repo_dir = snapshot_download(
        repo_id="TouradAi/ohada-vectorstore", repo_type="dataset"
    )

    zip_path = None
    for f in os.listdir(repo_dir):
        if f.endswith(".zip"):
            zip_path = os.path.join(repo_dir, f)
            break
    if not zip_path:
        raise FileNotFoundError(f"Aucun fichier .zip trouv√© dans {repo_dir}")

    os.makedirs(local_dir, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(local_dir)

    print(f"‚úÖ Contenu extrait dans {local_dir}:")
    for root, dirs, files in os.walk(local_dir):
        for file in files:
            print(f"  - {os.path.join(root, file)}")

    return local_dir


def load_vectorstore(local_dir):
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
    )
    vectorstore = Chroma(
        persist_directory=local_dir, embedding_function=embedding_model
    )
    return vectorstore


def setup_retriever(vectorstore):
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    return retriever


def setup_prompt():
    prompt = ChatPromptTemplate.from_template("""
Tu es **OHADA AI**, un assistant juridique sp√©cialis√© en droit OHADA.

### üéØ OBJECTIF
R√©pondre aux questions relatives au droit OHADA de mani√®re **fiable, pr√©cise et p√©dagogique**, en utilisant **uniquement** le CONTEXTE fourni.

### üß≠ R√àGLES FONDAMENTALES
1. **Source unique** : Tu ne peux utiliser que les informations pr√©sentes dans le CONTEXTE.  
   - Si une information ne s‚Äôy trouve pas, tu ne l‚Äôinventes pas.

2. **Exactitude juridique** :
   - Si tu cites un article ou un extrait pr√©sent dans le CONTEXTE, tu dois le **reproduire mot pour mot**.
   - Tu ne reformules jamais un texte juridique cit√©.

3. **Absence d‚Äôinformation suffisante** :
   - Si le CONTEXTE ne permet pas de r√©pondre pleinement, tu dis calmement :
     > "Je n'ai pas suffisamment d'informations dans les documents disponibles pour r√©pondre pr√©cis√©ment √† cette question."

4. **Questions hors droit OHADA ou conversationnelles** :
   - R√©pond de mani√®re **polie, naturelle et bienveillante**.
   - Rappelle subtilement que ton domaine est le droit OHADA.
   - Exemple de ton :
     > "Je peux discuter avec vous, mais je suis principalement con√ßu pour r√©pondre aux questions concernant le droit OHADA. N'h√©sitez pas √† m'en poser une üôÇ"

### ‚ú® STYLE DE R√âPONSE
- Clair, structur√©, et simple √† comprendre.
- Tu peux reformuler l√©g√®rement la question pour clarifier, mais **pas besoin d‚Äôannoncer que tu reformules**.
- Pas de ton professoral, pas de justification inutile.
- Objectif : **efficace, naturel, humain.**

---

### ‚ùì QUESTION
{question}

### üìö CONTEXTE (seule source autoris√©e)
{context}
""")
    return prompt



def setup_llm():
    llm = ChatOpenAI(
        api_key=SecretStr(os.environ.get("OPENROUTER_API_KEY") or ""),
        base_url="https://openrouter.ai/api/v1",
        model="qwen/qwen-2.5-72b-instruct",
        temperature=0.1,
        model_kwargs={"max_completion_tokens": 1000},
    )
    return llm


def setup_rag_chain(retriever, prompt, llm):
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain


def generate_answer_stream(question: str, rag_chain):
    """
    G√©n√®re une r√©ponse au format streaming depuis la cha√Æne RAG.
    Yields: texte progressif (string)
    """
    if not question.strip():
        yield "Veuillez poser une question."
        return

    try:
        streamed_text = ""
        for chunk in rag_chain.stream(question):
            streamed_text += chunk
            yield streamed_text
    except Exception as e:
        yield f"Erreur lors de la g√©n√©ration de la r√©ponse : {str(e)}"


# Global Initialization
local_dir = setup_vectorstore()
vectorstore = load_vectorstore(local_dir)
retriever = setup_retriever(vectorstore)
prompt = setup_prompt()
llm = setup_llm()
rag_chain = setup_rag_chain(retriever, prompt, llm)
